# Skipper Architecture

## ğŸ“ Folder Structure

```
skipper/
â”œâ”€â”€ core/                          # Core application logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                    # Main application orchestrator
â”‚   â””â”€â”€ config_manager.py         # Configuration management
â”‚
â”œâ”€â”€ vision/                        # Computer vision & face tracking
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ face_detector.py          # Hailo face detection
â”‚   â”œâ”€â”€ face_recognizer.py        # Face recognition & embeddings
â”‚   â”œâ”€â”€ face_tracker.py           # Face tracking logic
â”‚   â”œâ”€â”€ camera_manager.py         # Camera abstraction (single/dual)
â”‚   â””â”€â”€ enrollment.py             # Face enrollment system
â”‚
â”œâ”€â”€ motors/                        # Motor control & movement
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ klipper_client.py         # Klipper API client
â”‚   â”œâ”€â”€ movement_controller.py    # High-level movement logic
â”‚   â”œâ”€â”€ calibration.py            # Motor calibration tools
â”‚   â””â”€â”€ face_follower.py          # Face following behavior
â”‚
â”œâ”€â”€ sensors/                       # Environmental sensors
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ i2c_multiplexer.py        # TCA9548A multiplexer
â”‚   â”œâ”€â”€ temperature_humidity.py   # SHT3x sensor
â”‚   â”œâ”€â”€ pressure.py               # BMP280 sensor
â”‚   â””â”€â”€ sensor_manager.py         # Unified sensor interface
â”‚
â”œâ”€â”€ ai/                            # AI/LLM integration (future)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm_client.py             # LLM API client (Ollama, OpenAI, etc)
â”‚   â”œâ”€â”€ context_manager.py        # Conversation context
â”‚   â”œâ”€â”€ prompt_templates.py       # System prompts
â”‚   â””â”€â”€ embeddings.py             # Vector embeddings for RAG
â”‚
â”œâ”€â”€ audio/                         # Audio I/O (future voice features)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ speech_to_text.py         # STT interface
â”‚   â”œâ”€â”€ text_to_speech.py         # TTS interface
â”‚   â””â”€â”€ audio_device.py           # Audio hardware abstraction
â”‚
â”œâ”€â”€ utils/                         # Shared utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging.py                # Logging configuration
â”‚   â”œâ”€â”€ validators.py             # Input validation
â”‚   â”œâ”€â”€ image_utils.py            # Image processing helpers
â”‚   â””â”€â”€ system_info.py            # System diagnostics
â”‚
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ default.yaml              # Default configuration
â”‚   â”œâ”€â”€ camera.yaml               # Camera settings
â”‚   â”œâ”€â”€ motors.yaml               # Motor parameters
â”‚   â”œâ”€â”€ sensors.yaml              # Sensor configuration
â”‚   â””â”€â”€ printer.cfg.example       # Klipper config template
â”‚
â”œâ”€â”€ models/                        # ML models & weights
â”‚   â”œâ”€â”€ hailo/                    # Hailo model files
â”‚   â”‚   â””â”€â”€ scrfd_10g.hef
â”‚   â””â”€â”€ face_db/                  # Face recognition database
â”‚       â””â”€â”€ faces_db.pkl
â”‚
â”œâ”€â”€ data/                          # Runtime data & logs
â”‚   â”œâ”€â”€ logs/                     # Application logs
â”‚   â”œâ”€â”€ enrollment/               # Enrollment photos
â”‚   â””â”€â”€ cache/                    # Temporary cache
â”‚
â”œâ”€â”€ tests/                         # Unit & integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_vision.py
â”‚   â”œâ”€â”€ test_motors.py
â”‚   â”œâ”€â”€ test_sensors.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ scripts/                       # Standalone utility scripts
â”‚   â”œâ”€â”€ capture_enrollment.py     # Face enrollment capture
â”‚   â”œâ”€â”€ calibrate_motors.py       # Motor calibration
â”‚   â”œâ”€â”€ test_hardware.py          # Hardware diagnostics
â”‚   â””â”€â”€ setup/                    # Setup scripts
â”‚       â”œâ”€â”€ install_dependencies.sh
â”‚       â””â”€â”€ configure_system.sh
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # This file
â”‚   â”œâ”€â”€ API.md                    # API documentation
â”‚   â”œâ”€â”€ HARDWARE.md               # Hardware setup
â”‚   â””â”€â”€ DEVELOPMENT.md            # Development guide
â”‚
â”œâ”€â”€ web/                           # Web interface (Flask/FastAPI)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                    # Web server
â”‚   â”œâ”€â”€ routes/                   # API routes
â”‚   â”œâ”€â”€ static/                   # CSS, JS, images
â”‚   â””â”€â”€ templates/                # HTML templates
â”‚
â”œâ”€â”€ main.py                        # Main entry point
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ .env.example                   # Environment variables template
```

---

## ğŸ¯ Design Principles

### 1. **Separation of Concerns**
Each module has a single, well-defined responsibility:
- `vision/` - Everything related to cameras and face detection
- `motors/` - All motor control and movement logic
- `sensors/` - Environmental sensor reading and management
- `ai/` - LLM and AI-related features (future-ready)

### 2. **Abstraction Layers**
```
High-level API â†’ Business Logic â†’ Hardware Abstraction â†’ Hardware Drivers
```

Example:
```
face_follower.py â†’ movement_controller.py â†’ klipper_client.py â†’ Klipper Firmware
```

### 3. **Dependency Injection**
Components receive dependencies through constructors, making testing easier:

```python
class FaceFollower:
    def __init__(self, detector, tracker, motor_controller):
        self.detector = detector
        self.tracker = tracker
        self.motors = motor_controller
```

### 4. **Configuration Management**
All configuration in YAML files, no hardcoded values:
- Environment-specific configs (dev, prod)
- Easy parameter tuning without code changes
- Version controlled configuration

### 5. **Future-Ready AI Integration**
Pre-planned `ai/` module structure for:
- **LLM Integration**: Chat with the robot about what it sees
- **Context Awareness**: Remember faces, conversations, preferences
- **RAG (Retrieval-Augmented Generation)**: Search through face database with natural language
- **Multimodal AI**: Combine vision, sensor data, and language

---

## ğŸ”„ Data Flow

### Face Tracking Flow
```
Camera â†’ FaceDetector (Hailo) â†’ FaceRecognizer â†’ FaceTracker â†’ FaceFollower â†’ MotorController â†’ Klipper
```

### Sensor Reading Flow
```
TCA9548A â†’ SensorManager â†’ [SHT3x, BMP280] â†’ Data Aggregation â†’ Storage/Display/AI
```

### Future LLM Integration Flow
```
User Query â†’ LLM Client â†’ Context Manager â†’ [Vision Data, Sensor Data, Face DB] â†’ LLM Response
```

Example:
- User: "Who is in front of me?"
- System: Vision â†’ Face Recognition â†’ Context â†’ LLM â†’ "I see Achuthan standing 2 meters away"

---

## ğŸ§© Module Details

### `core/` - Application Core
**Purpose**: Orchestrate all subsystems, manage lifecycle

**Key Classes**:
- `Application` - Main app coordinator
- `ConfigManager` - Load/validate configs

**Responsibilities**:
- Initialize all subsystems
- Handle graceful shutdown
- Coordinate between modules
- Global error handling

---

### `vision/` - Computer Vision
**Purpose**: All camera and face-related functionality

**Key Classes**:
- `HailoFaceDetector` - SCRFD face detection
- `FaceRecognizer` - Template matching, embeddings
- `FaceTracker` - Track faces across frames
- `CameraManager` - Manage single/dual cameras
- `EnrollmentManager` - Face enrollment pipeline

**Hardware**:
- Hailo-8L HAT (AI accelerator)
- IMX708 CSI cameras

**Data Flow**:
```python
camera.capture() â†’ detector.detect() â†’ recognizer.identify() â†’ tracker.update()
```

---

### `motors/` - Movement Control
**Purpose**: Motor control and face following

**Key Classes**:
- `KlipperClient` - Communicate with Klipper API
- `MovementController` - High-level movement commands
- `CalibrationManager` - Motor calibration tools
- `FaceFollower` - Face following behavior

**Hardware**:
- Stepper motors (pan/tilt)
- Klipper firmware on MCU

**Example Usage**:
```python
motor_controller.move_to(pan=45, tilt=15)
face_follower.follow(face_position)
```

---

### `sensors/` - Environmental Sensing
**Purpose**: Read and manage I2C sensors

**Key Classes**:
- `TCA9548A` - I2C multiplexer controller
- `SHT3xSensor` - Temperature & humidity
- `BMP280Sensor` - Temperature & pressure
- `SensorManager` - Unified sensor interface

**Hardware**:
- TCA9548A I2C multiplexer
- SHT3x (0x44, channel 0)
- BMP280 (0x76, channel 1)

**Example Usage**:
```python
sensor_manager.read_all()  # Returns all sensor data
sensor_manager.get_temperature()  # Average temperature
```

---

### `ai/` - AI & LLM (Future)
**Purpose**: Intelligent conversation and reasoning

**Planned Features**:

#### 1. **LLM Integration**
```python
class LLMClient:
    def chat(self, message, context):
        """Chat with multimodal context"""
        
    def describe_scene(self, vision_data, sensor_data):
        """Describe what the robot sees"""
```

#### 2. **Context Management**
- Remember previous interactions
- Track face appearances over time
- Build user profiles

#### 3. **RAG (Retrieval-Augmented Generation)**
```python
# Search face database with natural language
llm.query("Who did I meet last Tuesday?")
# â†’ Searches embeddings, finds faces, generates response
```

#### 4. **Multimodal Understanding**
Combine multiple data sources:
```python
context = {
    "vision": "Face detected: Achuthan",
    "sensors": "Temperature: 24.5Â°C, Humidity: 48%",
    "time": "2025-11-01 10:30",
    "history": "Last seen 2 hours ago"
}
llm.chat("What's the environment like?", context)
# â†’ "The room is comfortable at 24.5Â°C with Achuthan present."
```

#### 5. **Planned Integrations**
- **Ollama** - Local LLM (Llama 3.2, Mistral)
- **OpenAI API** - GPT-4, GPT-4 Vision
- **Anthropic Claude** - Advanced reasoning
- **Vector DB** - ChromaDB, Qdrant for embeddings

---

### `utils/` - Shared Utilities
**Purpose**: Common helper functions

**Modules**:
- `logging.py` - Structured logging
- `validators.py` - Input validation
- `image_utils.py` - Image processing
- `system_info.py` - System diagnostics

---

### `web/` - Web Interface
**Purpose**: Browser-based control and monitoring

**Features**:
- Live camera feed
- Face recognition display
- Sensor data dashboard
- Motor control panel
- Configuration interface

**Tech Stack**:
- Backend: Flask or FastAPI
- Frontend: HTML, CSS, JavaScript
- Real-time: WebSockets for live feed

---

## ğŸš€ Entry Points

### `main.py` - Main Application
```python
# Run full face tracking system
python3 main.py --mode tracking --cameras 0 1

# Single camera mode
python3 main.py --mode tracking --single-camera 0

# Web interface only
python3 main.py --mode web --port 5000
```

### `scripts/` - Utility Scripts
```bash
# Enroll new face
python3 scripts/capture_enrollment.py --name "Person" --camera 0

# Calibrate motors
python3 scripts/calibrate_motors.py

# Test hardware
python3 scripts/test_hardware.py --component sensors
```

---

## ğŸ“¦ Package Structure

Make `skipper` a proper Python package:

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="skipper",
    version="2.0.0",
    packages=find_packages(),
    install_requires=[
        "opencv-python",
        "numpy",
        "flask",
        "pyyaml",
        "smbus2",
        "picamera2",
    ],
    entry_points={
        "console_scripts": [
            "skipper=core.app:main",
            "skipper-enroll=scripts.capture_enrollment:main",
            "skipper-calibrate=scripts.calibrate_motors:main",
        ],
    },
)
```

Then install in development mode:
```bash
pip3 install -e .
```

Use clean imports:
```python
from skipper.vision import FaceDetector
from skipper.motors import MovementController
from skipper.sensors import SensorManager
```

---

## ğŸ§ª Testing Strategy

### Unit Tests (`tests/`)
```python
# tests/test_vision.py
def test_face_detection():
    detector = FaceDetector()
    faces = detector.detect(test_image)
    assert len(faces) > 0

# tests/test_motors.py
def test_movement_range():
    controller = MovementController()
    controller.move_to(pan=45, tilt=30)
    assert controller.get_position() == (45, 30)
```

### Integration Tests
```python
# tests/test_integration.py
def test_face_following_pipeline():
    """Test complete face detection â†’ tracking â†’ motor control"""
    camera = CameraManager()
    detector = FaceDetector()
    follower = FaceFollower()
    
    frame = camera.capture()
    faces = detector.detect(frame)
    follower.follow(faces[0])
    
    assert motor_controller.is_moving()
```

---

## ğŸ”® Future Enhancements

### Phase 1: Current (Face Tracking + Sensors)
- âœ… Hailo face detection
- âœ… Face recognition
- âœ… Motor control via Klipper
- âœ… Environmental sensors

### Phase 2: AI Integration
- ğŸ”„ LLM client (Ollama)
- ğŸ”„ Context-aware conversations
- ğŸ”„ Scene description
- ğŸ”„ Natural language queries

### Phase 3: Advanced AI
- â³ RAG with face database
- â³ Multimodal understanding (vision + sensors + language)
- â³ Predictive behavior (anticipate user needs)
- â³ Voice interaction (STT/TTS integration)

### Phase 4: Edge AI
- â³ On-device LLM (Llama.cpp, GGUF models on Hailo)
- â³ Real-time video understanding
- â³ Gesture recognition
- â³ Emotion detection

---

## ğŸ“Š Configuration Management

### Environment Variables (`.env`)
```bash
# Hardware
HAILO_DEVICE=/dev/hailo0
KLIPPER_HOST=localhost
KLIPPER_PORT=7125

# AI/LLM (future)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OPENAI_API_KEY=sk-...

# Paths
DATA_DIR=/home/pi/work/skipper/data
MODEL_DIR=/home/pi/work/skipper/models
```

### YAML Configs
```yaml
# config/camera.yaml
camera:
  resolution: [1640, 1232]
  framerate: 30
  format: RGB888
  dual_mode: true
  cameras: [0, 1]

# config/motors.yaml
motors:
  pan:
    limits: [-180, 180]
    speed: 50
    acceleration: 100
  tilt:
    limits: [-90, 90]
    speed: 40
    acceleration: 80

# config/sensors.yaml
sensors:
  multiplexer:
    address: 0x70
    bus: 1
  sht3x:
    channel: 0
    address: 0x44
  bmp280:
    channel: 1
    address: 0x76
```

---

## ğŸ”— Dependencies Between Modules

```
core/
â”œâ”€â”€ vision/ (uses)
â”œâ”€â”€ motors/ (uses)
â”œâ”€â”€ sensors/ (uses)
â”œâ”€â”€ ai/ (uses, future)
â””â”€â”€ utils/ (uses)

vision/
â”œâ”€â”€ utils/ (uses)
â””â”€â”€ models/ (reads)

motors/
â””â”€â”€ utils/ (uses)

sensors/
â””â”€â”€ utils/ (uses)

ai/ (future)
â”œâ”€â”€ vision/ (uses)
â”œâ”€â”€ sensors/ (uses)
â”œâ”€â”€ utils/ (uses)
â””â”€â”€ models/ (reads)
```

**Rule**: No circular dependencies. Dependencies flow downward.

---

## ğŸ“ Summary

This architecture is:
- **Modular**: Easy to add/remove features
- **Testable**: Clear boundaries for unit testing
- **Scalable**: Ready for LLM and AI integration
- **Maintainable**: Well-organized, easy to navigate
- **Professional**: Follows Python packaging best practices

The structure separates hardware concerns (vision, motors, sensors) from future AI features (ai/, audio/), making it easy to evolve the project as LLM integration becomes ready.
